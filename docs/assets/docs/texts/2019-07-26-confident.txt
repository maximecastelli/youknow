<p style="text-indent:0em">Over the last few years we have seen an increase in altered and fabricated content pretending to be truthful news items. Powerful software that enables image and sound falsification is now available to most punters, and who could blame them for using it. To counter this, platforms like <span class="text-note" data-note="39">‘Snopes’</span> have been put in place to help verify the authenticity and the source of information encountered online. A cat and mouse game ensues as machine learning applications become as entrenched in detecting overflows of falsified computer generated media as they are in creating it.</p>
<p style="text-indent:2em">Machine learning algorithms are now able to generate credible footages of one person with only a few digital samples of this person’s voice and face. That is, anyone can impersonate anyone - see <span class="text-note" data-note="40">‘deepfakes’</span>. Recently a Belgian socialist party created and posted on Twitter a ‘deep fake’ video of Donald Trump saying that he “had the balls to withdraw from the Paris climate agreement; <span class="text-note" data-note="41">and so should you”</span>. The montage wasn’t perfect, and the aim of the video wasn’t to trick its audience but to provoke it. Not least because the sentence is an entirely believable one, this demonstrates how meticulous our screen-based news consumption will have to become, and we probably can’t do it without assistance.</p>
<p style="text-indent:2em">With image and sound doctoring technology improving rapidly and often (especially in open source software), the authenticity of information will inevitably become more and more uncertain. The thrill of automating and fabricating media content will probably just as quickly be matched by tools for proofreading and fact checking, and AI software like AdVerif.ai is leading the way in this domain. Its systems are built to "detect phony stories, nudity, malware, and a host of other types of problematic content. AdVerif.ai currently works with content platforms and advertising networks in the United States and Europe that don’t want to be associated with false or potentially <span class="text-note" data-note="42">offensive stories.”</span></p> 
<p style="text-indent:2em">The BBC News Lab is aware of the threat of ML in the creation of fake content, and has collaborated on a few experiments with <span class="text-note" data-note="43">Full Fact</span>, an independent UK fact checking charity. In this brief collaboration, the BBC did not develop any tools similar to AdVerif.ai, but one of News Lab’s current research projects consists of using ML to detect citations in BBC news articles. Knowing the source of the information and ‘who said what’ in advance helps train the AI to recognise authenticity.</p>
<p style="text-indent:2em">Meanwhile, computer scientists from Microsoft are using what is called Machine Reading Comprehension (MRC) to help Artificial Intelligence systems understand the ‘framing’ of information. MRC allows AI systems to not only understand individual words but to draw connections between them and grasp the overall concept of the paragraph. Researchers also aim to detect the most complex human form of expressions like sarcasm using <span class="text-note" data-note="44">similar techniques</span>. To understand sarcasm, machines have to detect the contradiction within the phrasing; a mismatch between the overall sentiment of words (often positive) and the sarcastic inferall (often negative). Again, understanding a friction point like this one is key to the accuracy of any journalistic practices, as it is with the reading of any text.</p>
<p style="text-indent:2em">The increasing sophistication of fake content through machine learning algorithms may amplify the skepticism towards foreign news stories and non-Western technologies. By education in the West we are becoming more vigilant about the problematics of received information, and more media literate. With the ‘black box’ model of machine learning and the proliferation of local and partisan news sites, it is easy to imagine a digital dark age where information is ubiquitous but we only (or barely) believe what we can see and touch - as incredulous as a biblical ‘Doubting Thomas’. Ignoring the Christian moral of this <span class="text-note" data-note="45">reference</span>, one wonders if the grand meme of ‘fake news’ could be brought to more fruitful ends, by removing the cathartic conflation of entertainment and news generally, leaving more space for localised social interactions and diversified knowledge networks outside of centralised broadcasting models that no longer hold claims to truth at all.</p>
<p style="text-indent:2em">While this is somewhat extreme, the decentralisation of news production and consumption is at the very least a fundamental shift from the global propaganda battle that the USA and the USSR fought during the cold war, with cultures of 'truth' and the function of the press being one of the most practical divergences between the two superpowers.</p>
<p style="text-indent:2em">Since 1989 however, the fake news of autocracy and the fake news of biblical capitalism have congealed to become more or less indistinguishable forums, especially on forums. Whilst intentions to standards of truthfulness should not be dismissed lightly, it is clear that we are witnessing the dismantling of universalist claims to truth at the hands of the same parties that constructed them. Contemporary US American and Russian literature is full of dark age futurist fantasies where communication is <span class="text-note" data-note="46">more feudal than global</span>.</p>
<p style="text-indent:2em">Such expectant fantasies of the post-truth era leading to a future more bizarre and heterogeneous than the one modelled in the 20th century need not be brutal medieval re-runs though. Amongst other things, a lie can stimulate the brain into finding alternatives to a standardised reality. ‘Ignorance’ can empower ‘creativity’. It can enrich the plurality of one story and give birth to various fantasies: ‘Truth is not enough’ writes philosopher <span class="text-note" data-note="47">Luciano Floridi</span>. Inside a post-truth era, naivety allows us to make sense of the world and rethink its logic. At its most fake, fake news could potentially be seen as an error that catalyses serendipity.</p>
<p style="text-indent:2em">Existing formats of user-generated news feeds on social media and specialised news platforms still require the personal illustration of every item. Owing to the incentive-based architecture of these platforms, the persona becomes primary to the content as a resource, and the news anchor becomes a news influencer. On Instagram, members subscribe to the aesthetics of an account owner’s lifestyle, fully aware that the pictures seen on people’s profile aren’t indeed <span class="text-note" data-note="48">‘Insta’</span> (i.e. spontaneous). The lies are consumed as consciously as one applies a <span class="text-note" data-note="49">KeyGen</span> to pirated software. None of it is real, true, or unique; but as copy it hits the mark.</p>
<p style="text-indent:2em">The format of rolling news bypasses this personal identity economy altogether, by stitching together footage from various sources including ‘citizens’, external news organisations, freelance and in-house reporters. The shooter of each piece of footage is not a key detail to the news itself but their identities function as the metadata to the <span class="text-note" data-note="50">media</span>. The identity data embedded within each human-mounted sensor (i.e. a personal camera) assists in preventing fraud, and the overall framing is curated elsewhere, centrally or collaboratively. In BBC news programs, journalists hold much more of a protagonist position, putting prose to images and framing outsourced information as first point of call. Along with editors and producers they are the BBC’s human watermark of trust, quality and responsibility, and we are not predicting their obsolescence here.</p>
<p style="text-indent:2em">A French TV show called ‘Zapping’ pastes together selected parts of recent television shows. Its creator Patrick Menais demonstrates that images do not necessarily need presenters and can speak by themselves generating meaning through their agencement (placement in series). As an ideal proof of concept, he was fired from host channel CanalPlus for using this zapping technique to protest against the current director. The point Menais makes is that more meaning can actually be derived “from the interaction of two sequential shots than from a single shot in isolation”, referring to the Kuleshov theory of <span class="text-note" data-note="51">early avant-garde cinema</span>. Even though this montage technique seems to erase the original context of each shot, it can profit broader correlations and fact explanations if there <i>is already</i> indeed a link between seemingly unrelated footage. Linked reels may seem absurd or random at first reading, but would actually represent more accurately the asymmetry and entangled strands of geopolitical cause and effect.</p>
<br>
Up next: <i>Impartiality</i>